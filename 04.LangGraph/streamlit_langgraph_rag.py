import streamlit as st
import json
import os
from typing import TypedDict, Literal, Generator
from dotenv import load_dotenv

# LangSmith ì¶”ì  í™œì„±í™”
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_PROJECT"] = "Self-Correcting-RAG"

# LangChain ê´€ë ¨
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.schema import Document
from langchain.callbacks.base import BaseCallbackHandler

# LangGraph ê´€ë ¨
from langgraph.graph import StateGraph, END

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ğŸ§  Self-Correcting RAG",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 2rem;
    }
    .step-container {
        padding: 1rem;
        border-radius: 10px;
        margin: 1rem 0;
        border-left: 4px solid #667eea;
        background-color: #f8f9fa;
    }
    .success-box {
        padding: 1rem;
        border-radius: 10px;
        background-color: #d4edda;
        border-left: 4px solid #28a745;
        margin: 1rem 0;
    }
    .warning-box {
        padding: 1rem;
        border-radius: 10px;
        background-color: #fff3cd;
        border-left: 4px solid #ffc107;
        margin: 1rem 0;
    }
    .error-box {
        padding: 1rem;
        border-radius: 10px;
        background-color: #f8d7da;
        border-left: 4px solid #dc3545;
        margin: 1rem 0;
    }
    .stButton>button {
        width: 100%;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        color: white;
        font-weight: bold;
        border: none;
        padding: 0.75rem;
        border-radius: 10px;
    }
</style>
""", unsafe_allow_html=True)


# Streamlit Callback Handler for streaming
class StreamlitCallbackHandler(BaseCallbackHandler):
    def __init__(self, container):
        self.container = container
        self.text = ""

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)


# State ì •ì˜
class SelfCorrectingRAGState(TypedDict):
    original_question: str
    current_question: str
    question_quality: str
    question_rewrite_count: int
    search_query: str
    retrieved_docs: list
    retrieval_quality: str
    search_retry_count: int
    answer: str
    answer_quality: str
    problem_diagnosis: str
    max_retries: int
    total_iterations: int  # ì „ì²´ ë°˜ë³µ íšŸìˆ˜ ì¶”ì 
    answer_generation_count: int  # ë‹µë³€ ìƒì„± íšŸìˆ˜ ì¶”ì 


def init_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    if 'steps' not in st.session_state:
        st.session_state.steps = []
    if 'vectorstore' not in st.session_state:
        st.session_state.vectorstore = None
    if 'app' not in st.session_state:
        st.session_state.app = None


def setup_vectorstore(api_key: str):
    """ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •"""
    if st.session_state.vectorstore is not None:
        return st.session_state.vectorstore

    # ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    sample_data = [
        {
            "id": 1,
            "topic": "ê·¼ë¬´ì‹œê°„",
            "content": "í…Œí¬ë…¸ë°”ì˜ ê·¼ë¬´ì‹œê°„ì€ í‰ì¼ ì˜¤ì „ 9ì‹œë¶€í„° ì˜¤í›„ 6ì‹œê¹Œì§€ì…ë‹ˆë‹¤. ì ì‹¬ì‹œê°„ì€ 12ì‹œë¶€í„° 1ì‹œê¹Œì§€ 1ì‹œê°„ì…ë‹ˆë‹¤."
        },
        {
            "id": 2,
            "topic": "íœ´ê°€ì •ì±…",
            "content": "ì—°ì°¨ëŠ” ì…ì‚¬ í›„ 1ë…„ì°¨ì— 15ì¼ì´ ì œê³µë˜ë©°, ë§¤ë…„ 1ì¼ì”© ì¶”ê°€ë˜ì–´ ìµœëŒ€ 25ì¼ê¹Œì§€ ì œê³µë©ë‹ˆë‹¤."
        },
        {
            "id": 3,
            "topic": "ë³µë¦¬í›„ìƒ",
            "content": "í…Œí¬ë…¸ë°”ëŠ” 4ëŒ€ë³´í—˜, í‡´ì§ì—°ê¸ˆ, ê±´ê°•ê²€ì§„, ê²½ì¡°ì‚¬ ì§€ì›, ìê¸°ê³„ë°œë¹„ ì§€ì› ë“± ë‹¤ì–‘í•œ ë³µë¦¬í›„ìƒì„ ì œê³µí•©ë‹ˆë‹¤."
        },
        {
            "id": 4,
            "topic": "ì¬íƒê·¼ë¬´",
            "content": "ì£¼ 2íšŒ ì¬íƒê·¼ë¬´ê°€ ê°€ëŠ¥í•˜ë©°, ì‚¬ì „ì— íŒ€ì¥ ìŠ¹ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤."
        },
        {
            "id": 5,
            "topic": "ê¸‰ì—¬",
            "content": "ê¸‰ì—¬ëŠ” ë§¤ì›” 25ì¼ì— ì§€ê¸‰ë˜ë©°, ì„±ê³¼ê¸‰ì€ ì—° 2íšŒ(ì—¬ë¦„, ê²¨ìš¸) ì§€ê¸‰ë©ë‹ˆë‹¤."
        }
    ]

    documents = []
    for item in sample_data:
        doc = Document(
            page_content=f"ì£¼ì œ: {item['topic']}\në‚´ìš©: {item['content']}",
            metadata={"id": item["id"], "topic": item["topic"]}
        )
        documents.append(doc)

    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=api_key)

    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embeddings,
        collection_name="company_info"
    )

    st.session_state.vectorstore = vectorstore
    return vectorstore


def create_rag_graph(api_key: str, retriever, max_retries: int):
    """Self-Correcting RAG ê·¸ë˜í”„ ìƒì„±"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7, openai_api_key=api_key)

    # ë…¸ë“œ í•¨ìˆ˜ë“¤
    def evaluate_question(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]

        with st.status("ğŸ” ì§ˆë¬¸ í‰ê°€ ì¤‘...", expanded=True) as status:
            evaluation_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì´ ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{question}"

í‰ê°€ ê¸°ì¤€:
1. ì§ˆë¬¸ì´ ë¬´ì—‡ì„ ë¬»ëŠ”ì§€ ëª…í™•í•œê°€?
2. ë¬¸ë§¥ ì—†ì´ë„ ì´í•´ ê°€ëŠ¥í•œê°€?
3. íšŒì‚¬ ì •ë³´ì— ëŒ€í•œ ì§ˆë¬¸ì¸ê°€?

"good" ë˜ëŠ” "bad" ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
""".strip()

            response = llm.invoke(evaluation_prompt)
            quality = "good" if "good" in response.content.strip().lower() else "bad"

            if quality == "good":
                st.success(f"âœ… ì§ˆë¬¸ì´ ëª…í™•í•©ë‹ˆë‹¤: {question}")
                status.update(label="âœ… ì§ˆë¬¸ í‰ê°€ ì™„ë£Œ", state="complete")
            else:
                st.warning(f"âš ï¸ ì§ˆë¬¸ì´ ë¶ˆëª…í™•í•©ë‹ˆë‹¤: {question}")
                status.update(label="âš ï¸ ì§ˆë¬¸ ì¬ì‘ì„± í•„ìš”", state="running")

        return {**state, "question_quality": quality}

    def rewrite_question(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]

        with st.status("âœï¸ ì§ˆë¬¸ ì¬ì‘ì„± ì¤‘...", expanded=True) as status:
            rewrite_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{question}"

ì¬ì‘ì„± ì‹œ ê³ ë ¤ì‚¬í•­:
1. íšŒì‚¬ ì •ë³´ë¥¼ ë¬»ëŠ” ê²ƒì´ ëª…í™•í•˜ê²Œ ë“œëŸ¬ë‚˜ë„ë¡
2. ì• ë§¤í•œ í‘œí˜„ ì œê±°
3. êµ¬ì²´ì ì¸ ì •ë³´ ìš”ì²­ìœ¼ë¡œ ë³€í™˜

ì¬ì‘ì„±ëœ ì§ˆë¬¸ë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""
            response = llm.invoke(rewrite_prompt)
            rewritten = response.content.strip()

            st.info(f"ğŸ“ ì›ë³¸: {question}")
            st.success(f"âœ¨ ì¬ì‘ì„±: {rewritten}")
            status.update(label="âœ… ì§ˆë¬¸ ì¬ì‘ì„± ì™„ë£Œ", state="complete")

        return {
            **state,
            "current_question": rewritten,
            "question_rewrite_count": state.get("question_rewrite_count", 0) + 1
        }

    def retrieve_documents(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        query = state.get("search_query") or state["current_question"]

        with st.status("ğŸ“š ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...", expanded=True) as status:
            docs = retriever.invoke(query)

            st.write(f"**ê²€ìƒ‰ ì¿¼ë¦¬:** {query}")
            st.write(f"**ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜:** {len(docs)}")

            for i, doc in enumerate(docs, 1):
                with st.expander(f"ğŸ“„ ë¬¸ì„œ {i}: {doc.metadata.get('topic', 'N/A')}"):
                    st.write(doc.page_content)

            status.update(label="âœ… ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ", state="complete")

        return {**state, "retrieved_docs": docs, "search_query": query}

    def evaluate_retrieval(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]
        docs = state["retrieved_docs"]

        with st.status("âœ… ê²€ìƒ‰ ê²°ê³¼ í‰ê°€ ì¤‘...", expanded=True) as status:
            docs_content = "\n\n".join([f"ë¬¸ì„œ {i + 1}: {doc.page_content[:100]}..."
                                        for i, doc in enumerate(docs)])

            evaluation_prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì´ ê´€ë ¨ì„±ì´ ìˆëŠ”ì§€ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{question}"

ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
{docs_content}

í‰ê°€ ê¸°ì¤€:
1. ë¬¸ì„œì— ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆëŠ” ì •ë³´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ê°€?
2. ë¬¸ì„œì™€ ì§ˆë¬¸ì˜ ì£¼ì œê°€ ì¼ì¹˜í•˜ëŠ”ê°€?

"relevant" ë˜ëŠ” "irrelevant" ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""

            response = llm.invoke(evaluation_prompt)
            quality = "relevant" if "relevant" in response.content.strip().lower() else "irrelevant"

            if quality == "relevant":
                st.success("âœ… ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤!")
                status.update(label="âœ… ê²€ìƒ‰ í‰ê°€ ì™„ë£Œ", state="complete")
            else:
                st.warning("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ê´€ë ¨ì´ ì—†ìŠµë‹ˆë‹¤. ì¬ê²€ìƒ‰ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                status.update(label="âš ï¸ ì¬ê²€ìƒ‰ í•„ìš”", state="running")

        return {**state, "retrieval_quality": quality}

    def rewrite_search_query(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]
        previous_query = state.get("search_query", question)

        with st.status("ğŸ”„ ê²€ìƒ‰ ì¿¼ë¦¬ ì¬ì‘ì„± ì¤‘...", expanded=True) as status:
            rewrite_prompt = f"""ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì ì ˆí–ˆìŠµë‹ˆë‹¤. ë” ë‚˜ì€ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ìœ„í•´ ì¿¼ë¦¬ë¥¼ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.

ì›ë³¸ ì§ˆë¬¸: "{question}"
ì´ì „ ê²€ìƒ‰ ì¿¼ë¦¬: "{previous_query}"

ì¬ì‘ì„± ì‹œ ê³ ë ¤ì‚¬í•­:
1. í•µì‹¬ í‚¤ì›Œë“œ ê°•ì¡°
2. ë™ì˜ì–´ë‚˜ ê´€ë ¨ ìš©ì–´ í¬í•¨
3. ë” êµ¬ì²´ì ì¸ í‘œí˜„ ì‚¬ìš©

ì¬ì‘ì„±ëœ ê²€ìƒ‰ ì¿¼ë¦¬ë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""

            response = llm.invoke(rewrite_prompt)
            new_query = response.content.strip()

            st.info(f"ğŸ“ ì´ì „: {previous_query}")
            st.success(f"âœ¨ ì¬ì‘ì„±: {new_query}")
            status.update(label="âœ… ì¿¼ë¦¬ ì¬ì‘ì„± ì™„ë£Œ", state="complete")

        return {
            **state,
            "search_query": new_query,
            "search_retry_count": state.get("search_retry_count", 0) + 1
        }

    def generate_answer(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]
        docs = state["retrieved_docs"]
        context = "\n\n".join([doc.page_content for doc in docs])

        with st.status("ğŸ’¬ ë‹µë³€ ìƒì„± ì¤‘...", expanded=True) as status:
            answer_container = st.empty()

            answer_prompt = f"""ë‹¹ì‹ ì€ íšŒì‚¬ ì •ë³´ë¥¼ ì•ˆë‚´í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

            # ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
            streaming_llm = ChatOpenAI(
                model="gpt-4o-mini",
                temperature=0.7,
                streaming=True,
                openai_api_key=api_key
            )

            answer = ""
            for chunk in streaming_llm.stream(answer_prompt):
                if chunk.content:
                    answer += chunk.content
                    answer_container.markdown(answer)

            status.update(label="âœ… ë‹µë³€ ìƒì„± ì™„ë£Œ", state="complete")

        return {
            **state,
            "answer": answer,
            "answer_generation_count": state.get("answer_generation_count", 0) + 1
        }

    def evaluate_answer(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]
        answer = state["answer"]
        total_iterations = state.get("total_iterations", 0)

        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì²´í¬ (ì „ì²´ ì‹œìŠ¤í…œ ì•ˆì „ì¥ì¹˜)
        if total_iterations >= 20:
            st.warning("âš ï¸ ìµœëŒ€ ì²˜ë¦¬ íšŸìˆ˜ ë„ë‹¬. í˜„ì¬ ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return {**state, "answer_quality": "good", "total_iterations": total_iterations + 1}

        with st.status("â­ ë‹µë³€ í‰ê°€ ì¤‘...", expanded=True) as status:
            evaluation_prompt = f"""ë‹¤ìŒ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{question}"
ë‹µë³€: "{answer}"

í‰ê°€ ê¸°ì¤€:
1. ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•˜ëŠ”ê°€?
2. ì»¨í…ìŠ¤íŠ¸ì˜ ì •ë³´ë¥¼ ì •í™•íˆ ì‚¬ìš©í–ˆëŠ”ê°€?
3. ë‹µë³€ì´ êµ¬ì²´ì ì´ê³  ìœ ìš©í•œê°€?
4. í™˜ê°(hallucination)ì´ ì—†ëŠ”ê°€?

"good" ë˜ëŠ” "bad" ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”.
"""

            response = llm.invoke(evaluation_prompt)
            quality = "good" if "good" in response.content.strip().lower() else "bad"

            # ë‹µë³€ ìƒì„± íšŸìˆ˜ê°€ 2íšŒ ì´ìƒì´ë©´ ê°•ì œë¡œ good ì²˜ë¦¬ (ë¬´í•œë£¨í”„ ë°©ì§€)
            answer_gen_count = state.get("answer_generation_count", 0)
            if quality == "bad" and answer_gen_count >= 2:
                st.warning(f"âš ï¸ ë‹µë³€ ì¬ìƒì„± {answer_gen_count}íšŒ ì‹œë„. í˜„ì¬ ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                quality = "good"

            if quality == "good":
                st.success("âœ… ë‹µë³€ í’ˆì§ˆì´ ì¢‹ìŠµë‹ˆë‹¤!")
                status.update(label="âœ… ë‹µë³€ í‰ê°€ ì™„ë£Œ", state="complete")
            else:
                st.warning("âš ï¸ ë‹µë³€ í’ˆì§ˆì´ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                status.update(label="âš ï¸ ë‹µë³€ ê°œì„  í•„ìš”", state="running")

        return {**state, "answer_quality": quality, "total_iterations": total_iterations + 1}

    def diagnose_problem(state: SelfCorrectingRAGState) -> SelfCorrectingRAGState:
        question = state["current_question"]
        docs = state["retrieved_docs"]
        answer = state["answer"]

        with st.status("ğŸ”§ ë¬¸ì œ ì§„ë‹¨ ì¤‘...", expanded=True) as status:
            docs_summary = "\n".join([f"- {doc.metadata.get('topic', 'N/A')}" for doc in docs])

            diagnosis_prompt = f"""ë‹µë³€ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì–´ë””ì—ì„œ ë¬¸ì œê°€ ë°œìƒí–ˆëŠ”ì§€ ì§„ë‹¨í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: "{question}"
ê²€ìƒ‰ëœ ë¬¸ì„œë“¤:
{docs_summary}
ìƒì„±ëœ ë‹µë³€: "{answer}"

ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:
1. "question_issue" - ì§ˆë¬¸ ìì²´ì— ë¬¸ì œê°€ ìˆìŒ (ì• ë§¤í•˜ê±°ë‚˜ ë¶ˆëª…í™•)
2. "retrieval_issue" - ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ë¶€ì ì ˆí•¨
3. "generation_issue" - ë‹µë³€ ìƒì„± ê³¼ì •ì—ì„œ ë¬¸ì œ ë°œìƒ

ì§„ë‹¨ ê²°ê³¼ë§Œ ë‹µë³€í•˜ì„¸ìš” (question_issue, retrieval_issue, generation_issue ì¤‘ í•˜ë‚˜).
"""

            response = llm.invoke(diagnosis_prompt)
            diagnosis = response.content.strip().lower()

            if "question" in diagnosis:
                diagnosis = "question_issue"
                st.warning("âš ï¸ ì§ˆë¬¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì¬ì‘ì„±í•©ë‹ˆë‹¤.")
            elif "retrieval" in diagnosis:
                diagnosis = "retrieval_issue"
                st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
            else:
                diagnosis = "generation_issue"
                st.warning("âš ï¸ ë‹µë³€ ìƒì„±ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹µë³€ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.")

            status.update(label="âœ… ë¬¸ì œ ì§„ë‹¨ ì™„ë£Œ", state="complete")

        return {**state, "problem_diagnosis": diagnosis}

    # ë¼ìš°íŒ… í•¨ìˆ˜ë“¤
    def route_after_question_eval(state: SelfCorrectingRAGState) -> str:
        if state["question_quality"] == "good":
            return "retrieve"
        elif state.get("question_rewrite_count", 0) >= max_retries:
            return "retrieve"
        else:
            return "rewrite_question"

    def route_after_retrieval_eval(state: SelfCorrectingRAGState) -> str:
        if state["retrieval_quality"] == "relevant":
            return "generate"
        elif state.get("search_retry_count", 0) >= max_retries:
            return "generate"
        else:
            return "rewrite_query"

    def route_after_answer_eval(state: SelfCorrectingRAGState) -> str:
        answer_quality = state["answer_quality"]
        answer_gen_count = state.get("answer_generation_count", 0)

        if answer_quality == "good":
            return "end"
        else:
            # ë‹µë³€ì´ badì§€ë§Œ ì´ë¯¸ ì—¬ëŸ¬ ë²ˆ ìƒì„±í–ˆë‹¤ë©´ ì¢…ë£Œ
            if answer_gen_count >= 2:
                st.warning("âš ï¸ ë‹µë³€ ê°œì„  ì‹œë„ ì™„ë£Œ. í˜„ì¬ ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return "end"
            return "diagnose"

    def route_after_diagnosis(state: SelfCorrectingRAGState) -> str:
        diagnosis = state["problem_diagnosis"]
        max_retries_value = state.get("max_retries", 2)
        answer_gen_count = state.get("answer_generation_count", 0)

        # ì „ì²´ ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬
        total_iterations = state.get("total_iterations", 0)
        if total_iterations >= 20:
            st.error("âš ï¸ ì‹œìŠ¤í…œ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ë„ë‹¬. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            return "end"

        # ì¬ì‹œë„ íšŸìˆ˜ ì²´í¬ - ìµœëŒ€ íšŸìˆ˜ ì´ˆê³¼ ì‹œ ê°•ì œ ì¢…ë£Œ
        if diagnosis == "question_issue":
            if state.get("question_rewrite_count", 0) >= max_retries_value:
                st.warning(f"âš ï¸ ì§ˆë¬¸ ì¬ì‘ì„± ìµœëŒ€ íšŸìˆ˜({max_retries_value}íšŒ) ì´ˆê³¼. í˜„ì¬ ì§ˆë¬¸ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                return "retrieve"
            return "rewrite_question"
        elif diagnosis == "retrieval_issue":
            if state.get("search_retry_count", 0) >= max_retries_value:
                st.warning(f"âš ï¸ ê²€ìƒ‰ ì¬ì‹œë„ ìµœëŒ€ íšŸìˆ˜({max_retries_value}íšŒ) ì´ˆê³¼. í˜„ì¬ ê²°ê³¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
                return "generate"
            return "rewrite_query"
        else:  # generation_issue
            # ë‹µë³€ ì¬ìƒì„±ì€ 2íšŒê¹Œì§€ë§Œ í—ˆìš©
            if answer_gen_count >= 2:
                st.warning(f"âš ï¸ ë‹µë³€ ì¬ìƒì„± ìµœëŒ€ íšŸìˆ˜(2íšŒ) ì´ˆê³¼. í˜„ì¬ ë‹µë³€ìœ¼ë¡œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                return "end"
            return "generate"

    # ê·¸ë˜í”„ êµ¬ì„±
    workflow = StateGraph(SelfCorrectingRAGState)

    workflow.add_node("evaluate_question", evaluate_question)
    workflow.add_node("rewrite_question", rewrite_question)
    workflow.add_node("retrieve", retrieve_documents)
    workflow.add_node("evaluate_retrieval", evaluate_retrieval)
    workflow.add_node("rewrite_query", rewrite_search_query)
    workflow.add_node("generate", generate_answer)
    workflow.add_node("evaluate_answer", evaluate_answer)
    workflow.add_node("diagnose", diagnose_problem)

    workflow.set_entry_point("evaluate_question")

    workflow.add_conditional_edges(
        "evaluate_question",
        route_after_question_eval,
        {"retrieve": "retrieve", "rewrite_question": "rewrite_question"}
    )

    workflow.add_edge("rewrite_question", "evaluate_question")
    workflow.add_edge("retrieve", "evaluate_retrieval")

    workflow.add_conditional_edges(
        "evaluate_retrieval",
        route_after_retrieval_eval,
        {"generate": "generate", "rewrite_query": "rewrite_query"}
    )

    workflow.add_edge("rewrite_query", "retrieve")
    workflow.add_edge("generate", "evaluate_answer")

    workflow.add_conditional_edges(
        "evaluate_answer",
        route_after_answer_eval,
        {"end": END, "diagnose": "diagnose"}
    )

    workflow.add_conditional_edges(
        "diagnose",
        route_after_diagnosis,
        {
            "rewrite_question": "rewrite_question",
            "rewrite_query": "rewrite_query",
            "generate": "generate",
            "retrieve": "retrieve",
            "end": END
        }
    )

    return workflow.compile()


def main():
    init_session_state()

    # í—¤ë”
    st.markdown('<h1 class="main-header">ğŸ§  Self-Correcting RAG System</h1>', unsafe_allow_html=True)
    st.markdown("### ìŠ¤ìŠ¤ë¡œ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ê°œì„ í•˜ëŠ” ì§€ëŠ¥í˜• RAG ì‹œìŠ¤í…œ")

    # ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")

        api_key = st.text_input("OpenAI API Key", type="password", help="OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")

        with st.expander("ğŸ” LangSmith ì¶”ì  (ì„ íƒì‚¬í•­)"):
            langsmith_api_key = st.text_input(
                "LangSmith API Key",
                type="password",
                help="LangSmith ì¶”ì ì„ ìœ„í•œ API í‚¤ (https://smith.langchain.com)"
            )
            if langsmith_api_key:
                os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
                st.success("âœ… LangSmith ì¶”ì ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.info("í”„ë¡œì íŠ¸ ì´ë¦„: Self-Correcting-RAG")
            else:
                os.environ["LANGCHAIN_TRACING_V2"] = "false"

        max_retries = st.slider("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜", 1, 5, 2)

        st.divider()

        st.header("ğŸ“Š ì‹œìŠ¤í…œ ì •ë³´")
        st.info("""
        **Self-Correcting RAG**ëŠ” ê° ë‹¨ê³„ë§ˆë‹¤ í’ˆì§ˆì„ ìë™ìœ¼ë¡œ ê²€ì¦í•˜ê³ , 
        ë¬¸ì œê°€ ë°œê²¬ë˜ë©´ ìŠ¤ìŠ¤ë¡œ ê°œì„ í•˜ëŠ” ê³ ê¸‰ RAG ì‹œìŠ¤í…œì…ë‹ˆë‹¤.

        **ì£¼ìš” ê¸°ëŠ¥:**
        - âœ… ì§ˆë¬¸ ëª…í™•ì„± ìë™ í‰ê°€
        - ğŸ”„ ë¶€ì ì ˆí•œ ì§ˆë¬¸ ìë™ ì¬ì‘ì„±
        - ğŸ¯ ê²€ìƒ‰ ê²°ê³¼ ê´€ë ¨ì„± í‰ê°€
        - ğŸ” ê²€ìƒ‰ ì¿¼ë¦¬ ìµœì í™”
        - ğŸ’¬ ë‹µë³€ í’ˆì§ˆ ìë™ ê²€ì¦
        - ğŸ”§ ë¬¸ì œ ì§„ë‹¨ ë° ìë™ ìˆ˜ì •
        """)

        if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ì—­ ì´ˆê¸°í™”"):
            st.session_state.messages = []
            st.session_state.steps = []
            st.rerun()

    # ë©”ì¸ ì˜ì—­
    if not api_key:
        st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ OpenAI API Keyë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.stop()

    # ë²¡í„° ìŠ¤í† ì–´ ì„¤ì •
    try:
        with st.spinner("ğŸ“š ë²¡í„° ìŠ¤í† ì–´ ì´ˆê¸°í™” ì¤‘..."):
            vectorstore = setup_vectorstore(api_key)
            retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 3})
        st.success("âœ… ì‹œìŠ¤í…œì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
    except Exception as e:
        st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
        st.stop()

    # ì§ˆë¬¸ ì…ë ¥
    st.divider()

    col1, col2 = st.columns([4, 1])
    with col1:
        user_question = st.text_input(
            "ğŸ’­ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”",
            placeholder="ì˜ˆ: í…Œí¬ë…¸ë°”ì˜ ê·¼ë¬´ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            key="question_input"
        )
    with col2:
        st.write("")  # ì •ë ¬ì„ ìœ„í•œ ê³µë°±
        st.write("")
        submit_button = st.button("ğŸš€ ì§ˆë¬¸í•˜ê¸°", use_container_width=True)

    # ì˜ˆì œ ì§ˆë¬¸
    st.caption("**ì˜ˆì œ ì§ˆë¬¸:**")
    example_cols = st.columns(3)
    with example_cols[0]:
        if st.button("ğŸ’¼ ê·¼ë¬´ì‹œê°„", use_container_width=True):
            st.session_state.question_input = "í…Œí¬ë…¸ë°”ì˜ ê·¼ë¬´ì‹œê°„ì€ ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
            st.rerun()
    with example_cols[1]:
        if st.button("ğŸ–ï¸ íœ´ê°€ì •ì±…", use_container_width=True):
            st.session_state.question_input = "ì—°ì°¨ëŠ” ëª‡ ì¼ì¸ê°€ìš”?"
            st.rerun()
    with example_cols[2]:
        if st.button("ğŸ  ì¬íƒê·¼ë¬´", use_container_width=True):
            st.session_state.question_input = "ì¬íƒê·¼ë¬´ê°€ ê°€ëŠ¥í•œê°€ìš”?"
            st.rerun()

    # ì§ˆë¬¸ ì²˜ë¦¬
    if submit_button and user_question:
        st.divider()
        st.markdown("### ğŸ”„ ì²˜ë¦¬ ê³¼ì •")

        try:
            # RAG ê·¸ë˜í”„ ìƒì„±
            app = create_rag_graph(api_key, retriever, max_retries)

            # ì´ˆê¸° ìƒíƒœ
            initial_state = {
                "original_question": user_question,
                "current_question": user_question,
                "question_quality": "",
                "question_rewrite_count": 0,
                "search_query": "",
                "retrieved_docs": [],
                "retrieval_quality": "",
                "search_retry_count": 0,
                "answer": "",
                "answer_quality": "",
                "problem_diagnosis": "",
                "max_retries": max_retries,
                "total_iterations": 0,
                "answer_generation_count": 0
            }

            # ì‹¤í–‰ (recursion_limit ì„¤ì •)
            result = app.invoke(
                initial_state,
                config={"recursion_limit": 50}  # ì¬ê·€ ì œí•œ ì¦ê°€
            )

            # ìµœì¢… ê²°ê³¼ í‘œì‹œ
            st.divider()
            st.markdown("### âœ¨ ìµœì¢… ë‹µë³€")

            st.markdown(f"""
            <div class="success-box">
                <h4>ğŸ’¬ {result['answer']}</h4>
            </div>
            """, unsafe_allow_html=True)

            # í†µê³„ ì •ë³´
            st.divider()
            st.markdown("### ğŸ“Š ì²˜ë¦¬ í†µê³„")

            col1, col2, col3, col4, col5 = st.columns(5)
            with col1:
                st.metric("ì§ˆë¬¸ ì¬ì‘ì„±", f"{result['question_rewrite_count']}íšŒ")
            with col2:
                st.metric("ê²€ìƒ‰ ì¬ì‹œë„", f"{result['search_retry_count']}íšŒ")
            with col3:
                st.metric("ë‹µë³€ ìƒì„±", f"{result.get('answer_generation_count', 1)}íšŒ")
            with col4:
                st.metric("ìµœì¢… í’ˆì§ˆ", "âœ… Good" if result['answer_quality'] == 'good' else "âš ï¸ Needs Work")
            with col5:
                st.metric("ê²€ìƒ‰ ë¬¸ì„œ", f"{len(result['retrieved_docs'])}ê°œ")

            # ìƒì„¸ ì •ë³´
            with st.expander("ğŸ” ìƒì„¸ ì •ë³´ ë³´ê¸°"):
                st.json({
                    "ì›ë³¸ ì§ˆë¬¸": result['original_question'],
                    "ìµœì¢… ì§ˆë¬¸": result['current_question'],
                    "ê²€ìƒ‰ ì¿¼ë¦¬": result['search_query'],
                    "ì§ˆë¬¸ í’ˆì§ˆ": result['question_quality'],
                    "ê²€ìƒ‰ í’ˆì§ˆ": result['retrieval_quality'],
                    "ë‹µë³€ í’ˆì§ˆ": result['answer_quality'],
                    "ì´ ë°˜ë³µ íšŸìˆ˜": result.get('total_iterations', 0),
                    "ë‹µë³€ ìƒì„± íšŸìˆ˜": result.get('answer_generation_count', 1)
                })

        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            st.exception(e)


if __name__ == "__main__":
    main()